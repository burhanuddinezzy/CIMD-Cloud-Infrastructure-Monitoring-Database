# This is a Telegraf configuration file. Telegraf is on the VM, Elastic Search is on my local laptop
# and the telegraf data is being sent to ES through a reverse SSH tunnel. 
# REM === Start reverse SSH tunnel silently in background ===
# start /b "" ssh -i "C:\Users\burha\.ssh\cimd_postgresql_key" -N -R 9200:localhost:9200 -o ServerAliveInterval=60 -o ServerAliveCountMax=3 ubuntu@40.233.74.182 >> "C:\Users\burha\Documents\ssh_tunnel_log.txt" 2>&1
# Using Grafana for data vsiualization
# Type: Elasticsearch, Name: elasticsearch-nginx (set as Default), URL: https://localhost:9200/, Authentication: Basic authentication (User: elastic, Password: configured), TLS: Skip TLS certificate validation (enabled), 
# Index name: nginx-access-logs-*, Pattern: No pattern, Time field name: @timestamp, Max concurrent Shard Requests: 5, Min time interval: 10s, Include Frozen Indices: not specified, Logs: Message field name: _source, Level field name: not specified, Timeout: not specified.

#GNU nano 6.2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         /etc/telegraf/telegraf.conf
# Agent Configuration
[agent]
  interval = "1m"
  flush_interval = "1m"
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_jitter = "0s"
  precision = ""
  debug = true
  quiet = false
  logfile = ""
  logtarget = "stderr"
  omit_hostname = false

# Global Tags
[global_tags]
  location_id = "550e8400-e29b-41d4-a716-446655440001"  # Generic location UUID
  server_id = "550e8400-e29b-41d4-a716-446655440000"    # Generic server UUID

# Input Plugins
[[inputs.cpu]]
  percpu = false
  totalcpu = true
# fieldinclude = ["usage_active"]   # <--- comment this out

[[inputs.disk]]
  fieldinclude = ["used_percent"]
  ignore_fs = ["tmpfs", "devtmpfs", "overlay", "aufs", "squashfs", "zfs", "fuse"]

[[inputs.diskio]]
  fieldinclude = ["reads", "writes", "read_bytes", "write_bytes"]

[[inputs.kernel]]
  fieldinclude = ["boot_time"]

[[inputs.mem]]
  fieldinclude = ["used_percent"]

[[inputs.system]]
  fieldinclude = ["uptime"]

[[inputs.net]]
  fieldinclude = ["bytes_recv", "bytes_sent", "err_in", "err_out"]

[[inputs.ping]]
  urls = ["8.8.8.8"]
  count = 1
  fieldinclude = ["average_response_ms"]


[[processors.rename]]
  namepass = ["mem"]
  [[processors.rename.replace]]
    field = "used_percent"
    dest = "used_percent_mem"

[[processors.rename]]
  namepass = ["disk"]
  [[processors.rename.replace]]
    field = "used_percent"
    dest = "disk_usage_percent"

[[processors.rename]]
  namepass = ["cpu"]
  [[processors.rename.replace]]
    field = "usage_active"
    dest = "cpu_usage"

[[processors.rename]]
  namepass = ["diskio"]
  [[processors.rename.replace]]
    field = "read_bytes"
    dest = "disk_read_throughput"
  [[processors.rename.replace]]
    field = "write_bytes"
    dest = "disk_write_throughput"

[[processors.rename]]
  namepass = ["net"]
  [[processors.rename.replace]]
    field = "bytes_recv"
    dest = "network_in_bytes"
  [[processors.rename.replace]]
    field = "bytes_sent"
    dest = "network_out_bytes"

[[processors.rename]]
  namepass = ["ping"]
  [[processors.rename.replace]]
    field = "average_response_ms"
    dest = "latency_in_ms"

[[processors.override]]
  namepass = ["cpu", "mem", "disk", "net"]
  tags = { "location_id" = "550e8400-e29b-41d4-a716-446655440001", "server_id" = "550e8400-e29b-41d4-a716-446655440000" }

[[processors.starlark]]
  namepass = ["cpu", "mem", "disk", "net"]
  source = '''
def apply(metric):
    if "uptime" in metric.fields:
        metric.fields["uptime_in_mins"] = int(metric.fields["uptime"] / 60)
    if "err_in" in metric.fields and "err_out" in metric.fields:
        metric.fields["error_count"] = int(metric.fields["err_in"] + metric.fields["err_out"])
    return metric
'''

[[inputs.tail]]
  files = ["/var/log/nginx/access.log"]
  from_beginning = false
  name_override = "nginx_logs"
  data_format = "grok"
  grok_patterns = ['%{IPORHOST:client} - - \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:http_version}" %{NUMBER:response_code} %{NUMBER:bytes} "%{DATA:referrer}" "%{DATA:agent}"']
  #grok_custom_patterns = '''
#COMBINED_LOG_FORMAT %{IPORHOST:client} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:http_version}" %{NUMBER:response_code} (?:%{NUMBER:bytes}|-) "%{DATA:referrer}" "%{DATA:agent}"
#'''

[[inputs.tail]]
  files = ["/var/log/nginx/error.log"]
  from_beginning = true
  name_override = "nginx_error_logs"
  data_format = "grok"
  grok_patterns = ['%{YEAR}/%{MONTHNUM}/%{MONTHDAY} %{TIME:timestamp} \[%{LOGLEVEL:level}\] %{NUMBER:pid}#%{NUMBER:tid}: \*%{NUMBER:connection_id} %{DATA:message}, client: %{IPORHOST:client}, server: %{IPORHOST:server}, request: "%{DATA:request}", host: "%{DATA:host}"(?:, referrer: "%{DATA:referrer}")?']
# Output Plugins

# FILE OUTPUT (for system metrics)
[[outputs.file]]
  namepass = ["cpu", "mem", "disk", "net", "ping", "diskio", "kernel", "system"]
  files = ["/tmp/telegraf_metrics.json"]
  data_format = "json"
  json_timestamp_units = "1s"

# FILE OUTPUT (for Nginx logs)
[[outputs.file]]
  namepass = ["nginx_logs"]
  files = ["/tmp/nginx_logs.json"]
  data_format = "json"

[[outputs.file]]
  namepass = ["nginx_error_logs"]
  files = ["/tmp/nginx_error_logs.json"]
  data_format = "json"
  
# ES OUTPUT FOR SYSTEM METRICS
[[outputs.elasticsearch]]
  namepass = ["cpu", "mem", "disk", "net", "ping", "diskio", "kernel", "system"]
  urls = ["https://localhost:9200"]
  username = "elastic"
  password = "URfJM+Olva2eTV+8=Q15"
  insecure_skip_verify = true
  index_name = "telegraf-system-metrics-%Y.%m.%d"

# For access logs
[[outputs.elasticsearch]]
  namepass = ["nginx_logs"]
  urls = ["https://localhost:9200"]
  username = "elastic"
  password = "URfJM+Olva2eTV+8=Q15"
  insecure_skip_verify = true
  index_name = "nginx-access-logs-%Y.%m.%d"
  data_format = "json"

# For error logs
[[outputs.elasticsearch]]
  namepass = ["nginx_error_logs"]
  urls = ["https://localhost:9200"]
  username = "elastic"
  password = "URfJM+Olva2eTV+8=Q15"
  insecure_skip_verify = true
  index_name = "nginx-error-logs-%Y.%m.%d"
  data_format = "json"


